### Project Overview

This project explores the application of Reinforcement Learning (RL) algorithms in two simulated grid environments: 
- **Empty Environment**: A simple grid where the agent's goal is to navigate to a specified location.
- **Key Environment**: A more complex scenario where the agent must first find a key before reaching the goal location, overcoming obstacles and potentially unlocking doors.

The RL algorithms implemented include Monte Carlo Control, SARSA, and Q-Learning, each subjected to a grid search for hyperparameter optimization. The objective is to find the best policy that enables the agent to accomplish its goal efficiently.

### Environment Setup

The environments are based on the `gym-minigrid` package, which provides a minimalistic gridworld environment for RL tasks. The setup includes installing necessary dependencies, initializing the environments, and wrapping them for compatibility with the algorithms used.

### Algorithm Implementation

The project implements three foundational RL algorithms:
- **Monte Carlo Control**: Utilizes episode sampling to estimate action-value functions and derives policies based on these estimations.
- **SARSA (State-Action-Reward-State-Action)**: An on-policy temporal difference (TD) learning method that updates value estimates based on the current policy.
- **Q-Learning**: An off-policy TD learning algorithm that seeks to find the optimal action-value function independently of the policy being followed.

Each algorithm's implementation is structured to accommodate the exploration-exploitation trade-off via an epsilon-greedy policy, with hyperparameters like discount factor (gamma), learning rate (alpha), and epsilon decay being tuned during the grid search phase.

### Grid Search for Hyperparameter Tuning

A systematic grid search is conducted for each algorithm to identify the set of hyperparameters that result in the best performance in terms of reward maximization and goal achievement. The search varies parameters such as epsilon (for exploration), gamma (discount factor), and alpha (learning rate) across predefined ranges.

### Evaluation and Inference

Post-training, the policies generated by each algorithm are evaluated to compare their effectiveness in the given environments. The evaluation metrics include average rewards, number of steps taken to reach the goal, and the success rate (percentage of episodes where the goal was reached). Additionally, visual demonstrations of the agent's performance under the best-found policies are provided through rendered videos.

### Project Files and Structure

The project is organized into several sections, each corresponding to specific components of the RL workflow:

1. **Environment Initialization**: Scripts for setting up the environments and necessary dependencies.
2. **Algorithm Implementation**: Separate modules for Monte Carlo, SARSA, and Q-Learning algorithms.
3. **Hyperparameter Tuning**: Scripts for conducting grid search across different hyperparameters for each algorithm.
4. **Evaluation**: Code for assessing the performance of derived policies, including functions for reward and step history plotting.
5. **Inference and Visualization**: Functions to demonstrate the agent's behavior in the environment using the best policies.

### Running the Project

To run the project, follow these steps:
1. Ensure all dependencies are installed as per the setup instructions.
2. Execute the environment setup scripts to initialize the environments.
3. Run the algorithm-specific scripts to train the agents using Monte Carlo, SARSA, or Q-Learning.
4. Perform hyperparameter tuning via grid search scripts to find the optimal settings for each algorithm.
5. Evaluate the performance of the derived policies using the evaluation scripts.
6. Visualize the agent's behavior in the environments using the inference and visualization scripts.

### Conclusion

This project provides a hands-on introduction to applying RL algorithms in simulated environments. Through systematic training, hyperparameter tuning, and evaluation, it demonstrates the processes involved in deriving effective policies for goal-directed behavior in RL contexts. The project highlights the importance of algorithm selection and hyperparameter optimization in achieving optimal performance in RL tasks.
